{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYV0AgabQG_p",
        "outputId": "61fdd63a-699f-4bed-a0db-fe3c7d9a2913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in training set:\n",
            " Diagnosis\n",
            "B    198\n",
            "M    120\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class distribution in validation set:\n",
            " Diagnosis\n",
            "B    88\n",
            "M    49\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load your dataset (replace 'your_data.csv' with the actual file path)\n",
        "data_path = 'wdbc.data'  # Assuming your file is named wdbc.data\n",
        "\n",
        "# Column names based on the .names file\n",
        "column_names = [\n",
        "    'ID', 'Diagnosis', 'Radius_mean', 'Texture_mean', 'Perimeter_mean', 'Area_mean', 'Smoothness_mean',\n",
        "    'Compactness_mean', 'Concavity_mean', 'Concave_points_mean', 'Symmetry_mean', 'Fractal_dimension_mean',\n",
        "    'Radius_se', 'Texture_se', 'Perimeter_se', 'Area_se', 'Smoothness_se', 'Compactness_se', 'Concavity_se',\n",
        "    'Concave_points_se', 'Symmetry_se', 'Fractal_dimension_se', 'Radius_worst', 'Texture_worst',\n",
        "    'Perimeter_worst', 'Area_worst', 'Smoothness_worst', 'Compactness_worst', 'Concavity_worst',\n",
        "    'Concave_points_worst', 'Symmetry_worst', 'Fractal_dimension_worst'\n",
        "]\n",
        "\n",
        "# Load data (comma-separated)\n",
        "df = pd.read_csv(data_path, header=None, names=column_names)\n",
        "\n",
        "# Step 2: Remove the 'ID' column since it's not useful for modeling\n",
        "df = df.drop(columns=['ID'])\n",
        "\n",
        "# Step 3: Define features (X) and labels (y)\n",
        "X = df.drop(columns=['Diagnosis'])  # All columns except 'Diagnosis'\n",
        "y = df['Diagnosis']  # Diagnosis is the target label\n",
        "\n",
        "# Step 4: Split into training + validation (80%) and test (20%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Split the training + validation into training (70%) and validation (30%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Calculate class distributions for training and validation sets\n",
        "class_distribution_train = y_train.value_counts()\n",
        "class_distribution_val = y_val.value_counts()\n",
        "\n",
        "# Step 7: Report the size of each class in the training and validation sets\n",
        "print(\"Class distribution in training set:\\n\", class_distribution_train)\n",
        "print(\"\\nClass distribution in validation set:\\n\", class_distribution_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "class LogisticRegressionSGD:\n",
        "    def __init__(self, learning_rate=0.01, batch_size=32, max_iters=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.max_iters = max_iters\n",
        "        self.weights = None\n",
        "\n",
        "    # Sigmoid function\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    # Compute the binary cross-entropy loss (negative log-likelihood)\n",
        "    def compute_loss(self, X, y):\n",
        "        N = len(y)\n",
        "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
        "        # Avoid log(0) by using small epsilon\n",
        "        epsilon = 1e-10\n",
        "        loss = -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
        "        return loss\n",
        "\n",
        "    # Compute the gradient of the loss function\n",
        "    def compute_gradient(self, X, y):\n",
        "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
        "        gradient = np.dot(X.T, (y_pred - y)) / len(y)\n",
        "        return gradient\n",
        "\n",
        "    # Mini-batch SGD for optimization\n",
        "    def fit(self, X, y):\n",
        "        # Initialize weights randomly from a standard Gaussian distribution\n",
        "        self.weights = np.random.randn(X.shape[1])\n",
        "\n",
        "        for i in range(self.max_iters):\n",
        "            # Shuffle the data before creating mini-batches\n",
        "            indices = np.arange(X.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            for batch_start in range(0, X.shape[0], self.batch_size):\n",
        "                X_batch = X[batch_start:batch_start + self.batch_size]\n",
        "                y_batch = y[batch_start:batch_start + self.batch_size]\n",
        "\n",
        "                # Compute gradient for the current mini-batch\n",
        "                gradient = self.compute_gradient(X_batch, y_batch)\n",
        "\n",
        "                # Update weights\n",
        "                self.weights -= self.learning_rate * gradient\n",
        "\n",
        "            # Optional: Print loss every 100 iterations\n",
        "            if i % 100 == 0:\n",
        "                loss = self.compute_loss(X, y)\n",
        "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # Predict binary labels for input data X\n",
        "    def predict(self, X):\n",
        "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
        "        return (y_pred >= 0.5).astype(int)\n",
        "\n",
        "# Generate synthetic data for binary classification\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(1000, 20)  # 1000 samples, 20 features\n",
        "y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Label is 1 if the sum of the first two features is positive\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Experiment with different learning rates and batch sizes\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "batch_sizes = [16, 32, 64]\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for bs in batch_sizes:\n",
        "        print(f\"\\nTraining model with learning rate = {lr} and batch size = {bs}\")\n",
        "        model = LogisticRegressionSGD(learning_rate=lr, batch_size=bs, max_iters=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on the test set\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "        print(f\"Precision: {precision:.2f}\")\n",
        "        print(f\"Recall: {recall:.2f}\")\n",
        "        print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmy4JSk0UbDR",
        "outputId": "1d056e0f-c052-477c-d0e9-cd7ed519f327"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with learning rate = 0.001 and batch size = 16\n",
            "Iteration 0, Loss: 1.8794\n",
            "Iteration 100, Loss: 0.8664\n",
            "Iteration 200, Loss: 0.4140\n",
            "Iteration 300, Loss: 0.2563\n",
            "Iteration 400, Loss: 0.1972\n",
            "Iteration 500, Loss: 0.1706\n",
            "Iteration 600, Loss: 0.1563\n",
            "Iteration 700, Loss: 0.1472\n",
            "Iteration 800, Loss: 0.1407\n",
            "Iteration 900, Loss: 0.1356\n",
            "Accuracy: 99.00%\n",
            "Precision: 0.98\n",
            "Recall: 1.00\n",
            "F1 Score: 0.99\n",
            "\n",
            "Training model with learning rate = 0.001 and batch size = 32\n",
            "Iteration 0, Loss: 2.6316\n",
            "Iteration 100, Loss: 1.8601\n",
            "Iteration 200, Loss: 1.2596\n",
            "Iteration 300, Loss: 0.8237\n",
            "Iteration 400, Loss: 0.5406\n",
            "Iteration 500, Loss: 0.3746\n",
            "Iteration 600, Loss: 0.2839\n",
            "Iteration 700, Loss: 0.2348\n",
            "Iteration 800, Loss: 0.2067\n",
            "Iteration 900, Loss: 0.1895\n",
            "Accuracy: 96.50%\n",
            "Precision: 0.95\n",
            "Recall: 0.98\n",
            "F1 Score: 0.96\n",
            "\n",
            "Training model with learning rate = 0.001 and batch size = 64\n",
            "Iteration 0, Loss: 1.8728\n",
            "Iteration 100, Loss: 1.5038\n",
            "Iteration 200, Loss: 1.1919\n",
            "Iteration 300, Loss: 0.9391\n",
            "Iteration 400, Loss: 0.7427\n",
            "Iteration 500, Loss: 0.5948\n",
            "Iteration 600, Loss: 0.4855\n",
            "Iteration 700, Loss: 0.4064\n",
            "Iteration 800, Loss: 0.3492\n",
            "Iteration 900, Loss: 0.3078\n",
            "Accuracy: 92.00%\n",
            "Precision: 0.89\n",
            "Recall: 0.95\n",
            "F1 Score: 0.91\n",
            "\n",
            "Training model with learning rate = 0.01 and batch size = 16\n",
            "Iteration 0, Loss: 1.8812\n",
            "Iteration 100, Loss: 0.1380\n",
            "Iteration 200, Loss: 0.1086\n",
            "Iteration 300, Loss: 0.0939\n",
            "Iteration 400, Loss: 0.0845\n",
            "Iteration 500, Loss: 0.0778\n",
            "Iteration 600, Loss: 0.0727\n",
            "Iteration 700, Loss: 0.0687\n",
            "Iteration 800, Loss: 0.0653\n",
            "Iteration 900, Loss: 0.0625\n",
            "Accuracy: 98.50%\n",
            "Precision: 0.98\n",
            "Recall: 0.99\n",
            "F1 Score: 0.98\n",
            "\n",
            "Training model with learning rate = 0.01 and batch size = 32\n",
            "Iteration 0, Loss: 1.6986\n",
            "Iteration 100, Loss: 0.1831\n",
            "Iteration 200, Loss: 0.1422\n",
            "Iteration 300, Loss: 0.1228\n",
            "Iteration 400, Loss: 0.1104\n",
            "Iteration 500, Loss: 0.1016\n",
            "Iteration 600, Loss: 0.0949\n",
            "Iteration 700, Loss: 0.0896\n",
            "Iteration 800, Loss: 0.0852\n",
            "Iteration 900, Loss: 0.0815\n",
            "Accuracy: 98.50%\n",
            "Precision: 0.98\n",
            "Recall: 0.99\n",
            "F1 Score: 0.98\n",
            "\n",
            "Training model with learning rate = 0.01 and batch size = 64\n",
            "Iteration 0, Loss: 1.8198\n",
            "Iteration 100, Loss: 0.2914\n",
            "Iteration 200, Loss: 0.1833\n",
            "Iteration 300, Loss: 0.1563\n",
            "Iteration 400, Loss: 0.1408\n",
            "Iteration 500, Loss: 0.1298\n",
            "Iteration 600, Loss: 0.1213\n",
            "Iteration 700, Loss: 0.1146\n",
            "Iteration 800, Loss: 0.1091\n",
            "Iteration 900, Loss: 0.1044\n",
            "Accuracy: 98.50%\n",
            "Precision: 0.98\n",
            "Recall: 0.99\n",
            "F1 Score: 0.98\n",
            "\n",
            "Training model with learning rate = 0.1 and batch size = 16\n",
            "Iteration 0, Loss: 0.5687\n",
            "Iteration 100, Loss: 0.0602\n",
            "Iteration 200, Loss: 0.0464\n",
            "Iteration 300, Loss: 0.0398\n",
            "Iteration 400, Loss: 0.0357\n",
            "Iteration 500, Loss: 0.0328\n",
            "Iteration 600, Loss: 0.0306\n",
            "Iteration 700, Loss: 0.0289\n",
            "Iteration 800, Loss: 0.0274\n",
            "Iteration 900, Loss: 0.0262\n",
            "Accuracy: 99.00%\n",
            "Precision: 0.99\n",
            "Recall: 0.99\n",
            "F1 Score: 0.99\n",
            "\n",
            "Training model with learning rate = 0.1 and batch size = 32\n",
            "Iteration 0, Loss: 1.1748\n",
            "Iteration 100, Loss: 0.0781\n",
            "Iteration 200, Loss: 0.0602\n",
            "Iteration 300, Loss: 0.0517\n",
            "Iteration 400, Loss: 0.0464\n",
            "Iteration 500, Loss: 0.0427\n",
            "Iteration 600, Loss: 0.0399\n",
            "Iteration 700, Loss: 0.0376\n",
            "Iteration 800, Loss: 0.0358\n",
            "Iteration 900, Loss: 0.0342\n",
            "Accuracy: 98.50%\n",
            "Precision: 0.98\n",
            "Recall: 0.99\n",
            "F1 Score: 0.98\n",
            "\n",
            "Training model with learning rate = 0.1 and batch size = 64\n",
            "Iteration 0, Loss: 1.0513\n",
            "Iteration 100, Loss: 0.1002\n",
            "Iteration 200, Loss: 0.0772\n",
            "Iteration 300, Loss: 0.0663\n",
            "Iteration 400, Loss: 0.0595\n",
            "Iteration 500, Loss: 0.0547\n",
            "Iteration 600, Loss: 0.0511\n",
            "Iteration 700, Loss: 0.0482\n",
            "Iteration 800, Loss: 0.0458\n",
            "Iteration 900, Loss: 0.0438\n",
            "Accuracy: 98.50%\n",
            "Precision: 0.98\n",
            "Recall: 0.99\n",
            "F1 Score: 0.98\n"
          ]
        }
      ]
    }
  ]
}