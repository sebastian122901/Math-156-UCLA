{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq4Psu3KPnJH",
        "outputId": "20032e4f-8c00-4a42-e961-ec59d991fd1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 0.678071322953969\n",
            "Iteration 100, Loss: 0.27541501519448713\n",
            "Iteration 200, Loss: 0.2102191347994054\n",
            "Iteration 300, Loss: 0.17999393020274776\n",
            "Iteration 400, Loss: 0.16148696951164646\n",
            "Iteration 500, Loss: 0.1485784134805005\n",
            "Iteration 600, Loss: 0.13886616460488407\n",
            "Iteration 700, Loss: 0.13118763415156043\n",
            "Iteration 800, Loss: 0.12490137054229311\n",
            "Iteration 900, Loss: 0.11961990147443118\n",
            "Accuracy: 99.00%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegressionSGD:\n",
        "    def __init__(self, learning_rate=0.01, batch_size=32, max_iters=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.max_iters = max_iters\n",
        "        self.weights = None\n",
        "\n",
        "    # Sigmoid function\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    # Compute the binary cross-entropy loss (negative log-likelihood)\n",
        "    def compute_loss(self, X, y):\n",
        "        N = len(y)\n",
        "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
        "        # Avoid log(0) by using small epsilon\n",
        "        epsilon = 1e-10\n",
        "        loss = -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
        "        return loss\n",
        "\n",
        "    # Compute the gradient of the loss function\n",
        "    def compute_gradient(self, X, y):\n",
        "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
        "        gradient = np.dot(X.T, (y_pred - y)) / len(y)\n",
        "        return gradient\n",
        "\n",
        "    # Mini-batch SGD for optimization\n",
        "    def fit(self, X, y):\n",
        "        # Initialize weights\n",
        "        self.weights = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.max_iters):\n",
        "            # Shuffle the data before creating mini-batches\n",
        "            indices = np.arange(X.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            for batch_start in range(0, X.shape[0], self.batch_size):\n",
        "                X_batch = X[batch_start:batch_start + self.batch_size]\n",
        "                y_batch = y[batch_start:batch_start + self.batch_size]\n",
        "\n",
        "                # Compute gradient for the current mini-batch\n",
        "                gradient = self.compute_gradient(X_batch, y_batch)\n",
        "\n",
        "                # Update weights\n",
        "                self.weights -= self.learning_rate * gradient\n",
        "\n",
        "            # Optional: Print loss every 100 iterations\n",
        "            if i % 100 == 0:\n",
        "                loss = self.compute_loss(X, y)\n",
        "                print(f\"Iteration {i}, Loss: {loss}\")\n",
        "\n",
        "    # Predict binary labels for input data X\n",
        "    def predict(self, X):\n",
        "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
        "        return (y_pred >= 0.5).astype(int)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate synthetic data for binary classification\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(100, 2)  # 100 samples, 2 features\n",
        "    y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Label is 1 if the sum of the features is positive\n",
        "\n",
        "    # Initialize logistic regression model\n",
        "    model = LogisticRegressionSGD(learning_rate=0.01, batch_size=10, max_iters=1000)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Predict labels on the training data\n",
        "    predictions = model.predict(X)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ]
}