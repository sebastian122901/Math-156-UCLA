
This repository contains a simple implementation of a two-layer perceptron designed for binary classification tasks, demonstrated on the XOR problem.

Features
Two-layer perceptron with an input layer, hidden layer, and output layer.
ReLU activation for the hidden layer and sigmoid activation for the output layer.
Forward and backward propagation.
Mini-batch Stochastic Gradient Descent (SGD) optimizer.
Cross-entropy loss function.
Cross-validation for performance estimation.
Performance metrics including precision, recall, and F1-score.

Requirements
Python 3.x
NumPy
scikit-learn
matplotlib
pickle
Dataset
The dataset used is a toy XOR dataset stored in xordata.pkl

Parameter Choices
Hidden Size (10): Provides enough capacity to learn the XOR problem without overfitting.
Learning Rate (0.01): Balances between speed of convergence and stability.
Epochs (5000): Ensures the model has ample opportunity to learn from the data.
Batch Size (64): Balances gradient noise and computational efficiency.
Activation Functions:
ReLU for hidden layer: Captures non-linearity and helps mitigate the vanishing gradient problem.
Sigmoid for output layer: Suitable for binary classification, squashing outputs to a probability between 0 and 1.
Cross-Entropy Loss: Appropriate for binary classification tasks.
Weight Initialization:
Random Initialization: Ensures symmetry breaking.
Seed Value (42): Ensures reproducibility of results.
Zero Initialization of Biases: Simplifies initialization without negatively impacting learning.


